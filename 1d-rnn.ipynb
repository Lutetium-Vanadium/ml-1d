{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import EvalScript.evalResult as evalResult\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "BASE = './Data/'\n",
    "START = 'START'\n",
    "STOP = 'STOP'\n",
    "labels = ['O', 'B-negative', 'I-negative', 'B-neutral', 'I-neutral', 'B-positive', 'I-positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabelledData:\n",
    "    def __init__(self, path):\n",
    "        self.sentences = []\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            current_sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    self.sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "                else:\n",
    "                    current_sentence.append(line)\n",
    "            if len(current_sentence):\n",
    "                self.sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelledData:\n",
    "    def __init__(self, labels, path = None, train=True):\n",
    "        self.sentences = []\n",
    "        self.label_to_idx = {labels[i]: i for i in range(len(labels))}\n",
    "        self.word_to_idx = {\"\": 0} # for zero-padding variable lengths later\n",
    "\n",
    "        if path == None:\n",
    "            return\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            current_sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '':\n",
    "                    self.sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "                else:\n",
    "                    current_sentence.append(tuple(line.rsplit(maxsplit=1)))\n",
    "            if len(current_sentence):\n",
    "                self.sentences.append(current_sentence)\n",
    "        \n",
    "        if train:\n",
    "            self.data = self.get_training_data()\n",
    "            #self.embeddings = np.random.randn(len(self.word_to_idx), 7)\n",
    "            self.embeddings = self.get_embeddings(labels)\n",
    "    \n",
    "    def get_embeddings(self, labels, k=100):\n",
    "        # attempt to get a good estimate on embeddings here using emission probabilities\n",
    "        state_counts = {label: 0 for label in labels}\n",
    "        word_counts = {}\n",
    "        emission_counts = {}\n",
    "        embeddings = np.zeros((len(self.word_to_idx), len(labels)))\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            for word, label in sentence:\n",
    "                word = word.lower()\n",
    "                emission_counts.setdefault((label, word), 0)\n",
    "                word_counts.setdefault(word, 0)\n",
    "                \n",
    "                emission_counts[(label, word)] += 1\n",
    "                word_counts[word] += 1\n",
    "                state_counts[label] += 1\n",
    "        \n",
    "        for word, word_idx in self.word_to_idx.items():\n",
    "            for label, label_idx in self.label_to_idx.items():\n",
    "                state_count = state_counts[label]\n",
    "                word_count = word_counts.get(word, k)\n",
    "                emission_count = emission_counts.get((label, word), k)\n",
    "                \n",
    "                embeddings[word_idx][label_idx] = np.exp(np.log(emission_count) - np.log(word_count + k)) \n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def get_training_data(self):\n",
    "        seq = []\n",
    "        labels = []\n",
    "        current_idx = 0\n",
    "        training_data = []\n",
    "        for sentence in self.sentences:\n",
    "            for word, label in sentence:\n",
    "                # make everything lowercase\n",
    "                word = word.lower()\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.word_to_idx[word] = current_idx\n",
    "                    current_idx += 1\n",
    "\n",
    "                seq.append(self.word_to_idx[word])\n",
    "                labels.append(self.label_to_idx[label])\n",
    "            \n",
    "            training_data.append((seq.copy(), labels.copy()))\n",
    "            seq = []\n",
    "            labels = []\n",
    "\n",
    "        return training_data \n",
    "    \n",
    "    def write_to_file(self, path):\n",
    "        with open(path, 'w', encoding='utf8') as f:\n",
    "            for sentence in self.sentences:\n",
    "                for data in sentence:\n",
    "                    print(*data, file=f)\n",
    "                print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, data: LabelledData, hidden_dim=4, learning_rate=0.05, num_epochs=10):\n",
    "        self.word2idx = data.word_to_idx\n",
    "        self.label2idx = data.label_to_idx\n",
    "        self.embeddings = data.embeddings\n",
    "        self.data = data.data\n",
    "        self.labels = list(self.label2idx.keys())\n",
    "        \n",
    "        # dimensions to keep track\n",
    "        self.vocab_size, self.embedding_dim = self.embeddings.shape\n",
    "        self.output_dim = len(self.label2idx)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_samples = len(self.data)\n",
    "        \n",
    "        # weights\n",
    "        self.wxh = np.random.rand(self.embedding_dim, self.hidden_dim)\n",
    "        self.whh = np.random.rand(self.hidden_dim, self.hidden_dim) \n",
    "        self.why = np.random.rand(self.hidden_dim, self.output_dim) \n",
    "        self.bh = np.random.rand(self.hidden_dim) \n",
    "        self.by = np.random.rand(self.output_dim) \n",
    "    \n",
    "    def pad_sequences(self):\n",
    "        max_sequence_len = 0\n",
    "        for sequence, label in self.data:\n",
    "            curr_len = len(sequence)\n",
    "            if curr_len > max_sequence_len:\n",
    "                max_sequence_len = curr_len\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i] = ((self.data[i][0] + [0] * (max_sequence_len - len(self.data[i][0]))), self.data[i][1])\n",
    "        \n",
    "        return max_sequence_len\n",
    "    \n",
    "    def forward(self, sequence, sequence_length):\n",
    "        embedded = self.embeddings[sequence]\n",
    "        hidden_states = np.zeros((sequence_length, self.hidden_dim))\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            if t == 0:\n",
    "                hidden_states[t] = np.tanh(np.dot(embedded[t], self.wxh) + self.bh)\n",
    "            else:\n",
    "                hidden_states[t] = np.tanh(np.dot(embedded[t], self.wxh) + np.dot(hidden_states[t-1], self.whh) + self.bh)\n",
    "            \n",
    "        output = np.dot(hidden_states, self.why) + self.by\n",
    "        return self.softmax(output), hidden_states\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        output = e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for sequence, label in self.data:\n",
    "                sequence_length = len(sequence)\n",
    "                output, hidden_states = self.forward(np.array(sequence), sequence_length)\n",
    "                label = np.array([np.eye(self.output_dim)[label] for label in label])\n",
    "                \n",
    "                episilon = 1e-10\n",
    "                output = np.clip(output, episilon, 1 - episilon)\n",
    "                loss = -np.sum(label * np.log(output))\n",
    "                total_loss += loss\n",
    "                \n",
    "                gradient_output = (output - label) \n",
    "                gradient_hidden_states = np.dot(gradient_output, self.why.T)\n",
    "                gradient_why = np.dot(hidden_states.T, gradient_output)\n",
    "                gradient_by = np.sum(gradient_output, axis=0)\n",
    "                \n",
    "                gradient_bh = np.zeros_like(self.bh)\n",
    "                gradient_wxh = np.zeros_like(self.wxh)\n",
    "                gradient_whh = np.zeros_like(self.whh)\n",
    "                \n",
    "                for t in range(sequence_length - 1, -1, -1):\n",
    "                    gradient_bh += gradient_hidden_states[t] * (1 - hidden_states[t]**2)\n",
    "                    gradient_wxh += np.outer(self.embeddings[sequence[t]], gradient_hidden_states[t] * (1 - hidden_states[t]**2))\n",
    "                \n",
    "                    if t > 0:\n",
    "                        gradient_whh += np.outer(hidden_states[t - 1], gradient_hidden_states[t] * (1 - hidden_states[t]**2))\n",
    "                        gradient_hidden_states[t - 1] += np.dot(gradient_hidden_states[t] * (1 - hidden_states[t]**2), self.whh.T)\n",
    "\n",
    "                max_gradient_value = 5.0  \n",
    "                gradient_wxh = np.clip(gradient_wxh, -max_gradient_value, max_gradient_value)\n",
    "                gradient_whh = np.clip(gradient_whh, -max_gradient_value, max_gradient_value)\n",
    "                gradient_by = np.clip(gradient_by, -max_gradient_value, max_gradient_value)\n",
    "                gradient_bh = np.clip(gradient_bh, -max_gradient_value, max_gradient_value)\n",
    "                gradient_why = np.clip(gradient_why, -max_gradient_value, max_gradient_value)\n",
    "\n",
    "                self.why -= self.learning_rate * gradient_why\n",
    "                self.by -= self.learning_rate * gradient_by\n",
    "                self.wxh -= self.learning_rate * gradient_wxh\n",
    "                self.whh -= self.learning_rate * gradient_whh\n",
    "                self.bh -= self.learning_rate * gradient_bh\n",
    "                    \n",
    "                    \n",
    "            avg_loss = total_loss / len(self.data)\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def predict(self, data):\n",
    "        res = LabelledData(self.labels, train=False)\n",
    "\n",
    "        for sentence in data.sentences:\n",
    "            sequence = []\n",
    "            for word in sentence:\n",
    "                embedded = self.word2idx.get(word.lower())\n",
    "                sequence.append(embedded) if embedded else sequence.append(0)\n",
    "            \n",
    "            output = self.forward(sequence, len(sequence))[0]\n",
    "            labels = []\n",
    "            for prediction in output:\n",
    "                labels.append(np.argmax(prediction))\n",
    "            \n",
    "            labels = [self.labels[label] for label in labels]\n",
    "            \n",
    "            res.sentences.append([(word, label) for word, label in zip(sentence, labels)])\n",
    "        \n",
    "        return res\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.6531\n",
      "Epoch 2/10, Loss: 3.8594\n",
      "Epoch 3/10, Loss: 3.4757\n",
      "Epoch 4/10, Loss: 3.3336\n",
      "Epoch 5/10, Loss: 3.2462\n",
      "Epoch 6/10, Loss: 3.1798\n",
      "Epoch 7/10, Loss: 3.1261\n",
      "Epoch 8/10, Loss: 3.0879\n",
      "Epoch 9/10, Loss: 3.0625\n",
      "Epoch 10/10, Loss: 3.0429\n",
      "============= ES =============\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 344\n",
      "\n",
      "#Correct Entity : 164\n",
      "Entity  precision: 0.4767\n",
      "Entity  recall: 0.7162\n",
      "Entity  F: 0.5724\n",
      "\n",
      "#Correct Sentiment : 125\n",
      "Sentiment  precision: 0.3634\n",
      "Sentiment  recall: 0.5459\n",
      "Sentiment  F: 0.4363\n",
      "==============================\n",
      "Epoch 1/10, Loss: 3.8368\n",
      "Epoch 2/10, Loss: 2.7203\n",
      "Epoch 3/10, Loss: 2.4646\n",
      "Epoch 4/10, Loss: 2.3620\n",
      "Epoch 5/10, Loss: 2.3117\n",
      "Epoch 6/10, Loss: 2.2542\n",
      "Epoch 7/10, Loss: 2.2185\n",
      "Epoch 8/10, Loss: 2.1888\n",
      "Epoch 9/10, Loss: 2.1748\n",
      "Epoch 10/10, Loss: 2.1616\n",
      "============= RU =============\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 374\n",
      "\n",
      "#Correct Entity : 124\n",
      "Entity  precision: 0.3316\n",
      "Entity  recall: 0.3188\n",
      "Entity  F: 0.3250\n",
      "\n",
      "#Correct Sentiment : 87\n",
      "Sentiment  precision: 0.2326\n",
      "Sentiment  recall: 0.2237\n",
      "Sentiment  F: 0.2280\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['ES', 'RU']:\n",
    "    train = LabelledData(labels=labels, path=BASE + dataset + '/train')\n",
    "    dev_in = UnlabelledData(BASE + dataset + '/dev.in')\n",
    "    #predicted.write_to_file(BASE + dataset + '/dev.p4.out')\n",
    "    \n",
    "    model = RNN(train)\n",
    "    model.train()\n",
    "    predicted = model.predict(dev_in)\n",
    "    predicted.write_to_file(BASE + dataset + '/dev.p4.out')\n",
    "    print(f'{f\" {dataset} \":=^30}')\n",
    "    evalResult.evaluate(BASE + dataset + '/dev.out', BASE + dataset + '/dev.p4.out')\n",
    "    print('='*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007ML-OdyoVhM6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
